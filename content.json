{"meta":{"title":"夏天换上冬装的Blog","subtitle":"https://github.com/uniteReady/uniteReady.github.io","description":"夏天换上冬装的博客","author":"夏天换上冬装","url":"https://uniteready.github.io","root":"/"},"posts":[{"tags":[],"title":"Hello World","date":"2020/07/01","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","permalink":"https://uniteready.github.io/2020/07/01/hello-world/","photos":[]},{"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://uniteready.github.io/tags/Javascript/"}],"title":"树形数据结构与扁平数据结构互转","date":"2019/12/19","text":"前言近日有需求做链路日志系统，从 elasticsearch 查询到的日志数据是扁平化结构，数据中包含层级关系的字段，前端按照层级展示。 所以需要对扁平数据做树形处理 数据源 下面是单条数据结构： { id: 1, pid: 0, name: '1/0'} id 为不重复索引值 pid 为父级 id 下面简单构造一个扁平数据结构数据源 let data = [ { id: 1, pid: 0, name: '1/0' }, { id: 2, pid: 1, name: '2/0' }, { id: 0, pid: -1, name: '0/-1' }, { id: 3, pid: 1, name: '3/1' }, { id: 4, pid: 3, name: '4/3' }, { id: 6, pid: 5, name: '6/5' }, { id: 5, pid: 0, name: '5/0' }, { id: 7, pid: 2, name: '7/2' }] 扁平数据结构=>树形数据结构 对于下面的使用方法，是有一个 bug 的，那就是数据源的根父节点(最外层的父节点)，必须在数据源的首个位置（索引为 0），所以在使用本方法时需要依据 id 做一次排序 // 先依据id做一次排序，确保根父节点，在数据源首位data.sort((a, b) => a.id - b.id)function flatToNested(data) { let temp = [], result = [] for (let i = 0; i < data.length; i++) { // 将当前item塞入temp中(因为这一步，所以需要对数据源做排序) temp[data[i].id] = data[i] // 判断temp中有没有当前item的父类 if (temp[data[i].pid]) { // 判断childrenn，不存在则赋值空数组 if (!temp[[data[i].pid]]['children']) { temp[[data[i].pid]]['children'] = [] } // 将当前item塞入父类children字段中 temp[[data[i].pid]]['children'].push(data[i]) } else { // 在temp中找不到父类，就将item塞入result中 result.push(data[i]) } } return result}flatToNested(data) 处理完的数据： let data = [ { id: 0, pid: -1, name: '0/-1', children: [ { id: 1, pid: 0, name: '1/0', children: [ { id: 2, pid: 1, name: '2/0', children: [ { id: 7, pid: 2, name: '7/2' } ] }, { id: 3, pid: 1, name: '3/1', children: [ { id: 4, pid: 3, name: '4/3' } ] } ] }, { id: 5, pid: 0, name: '5/0', children: [ { id: 6, pid: 5, name: '6/5' } ] } ] }] 树形数据结构=>扁平数据结构 目前的处理方法就是递归，因为不知道有多次层级，有新的想法请与我联系！ function NestedToFlat(data) { let result = [] for (let i = 0; i < data.length; i++) { result.push({ id: data[i]['id'], pid: data[i]['pid'], name: data[i]['name'] }) if (data[i]['children']) { result = result.concat(this.demo(data[i].children)) } } return result}","permalink":"https://uniteready.github.io/2019/12/19/%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84/","photos":[]},{"tags":[{"name":"hive","slug":"hive","permalink":"https://uniteready.github.io/tags/hive/"},{"name":"udf","slug":"udf","permalink":"https://uniteready.github.io/tags/udf/"}],"title":"UDFToTopN案例","date":"2018/06/30","text":"需求: 统计最热门的课程Top10http://bigdata.com/course/458655.html => 458655http://bigdata.com/course/458655/2.html?a=b&c=d => 458655_2解析后再统计课程Top10 1.使用 MockClassData生成数据package com.cj.bigdata.hive.hiveWork;import java.io.*;import java.util.Random;public class MockClassData { public static void main(String[] args) throws IOException { //课程号数组 String words[] = {\"123\",\"4354\",\"43541\",\"43542\",\"43543\",\"43544\",\"43545\",\"43546\",\"43547\",\"43548\"}; Random random = new Random(); //文件输出流对象 BufferedWriter bufferedWriter = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(new File(\"data/input/classData.txt\")))); //for循环随机生成数据 for (int i = 0; i < 30000; i++) { bufferedWriter.write(\"http://bigdata.com/course/\" + words[random.nextInt(words.length)]); if(random.nextInt(4) != 0){ bufferedWriter.write(\"/\" + random.nextInt(3) + \".html?a=b&c=d\"); bufferedWriter.newLine(); }else { bufferedWriter.write(\".html\"); bufferedWriter.newLine(); } } bufferedWriter.flush(); bufferedWriter.close(); }} 2.创建表CREATE TABLE `default.class`( `class_str` string) 3.将数据upload服务器并import到table classload data local inpath '/home/jackson/data/classData.txt' overwrite into table class;#检查数据select * from class limit 10; 4.编写UDFParseClassCodepackage com.cj.bigdata.hive.hiveWork;import org.apache.hadoop.hive.ql.exec.UDF;public class UDFParseClassCode extends UDF { /*** * * http://bigdata.com/course/4354/2.html?a=b&c=d * http://bigdata.com/course/43548.html */ public String evaluate(String str){ StringBuffer buffer = new StringBuffer(); String[] splits01 = str.split(\".html\"); //http://bigdata.com/course/4354/2或http://bigdata.com/course/43548 String splits02 = splits01[0]; String[] splits03 = splits02.split(\"/\"); //用split后的的最后一块的长度来判断是//http://bigdata.com/course/4354/2还是http://bigdata.com/course/43548 if(splits03[splits03.length - 1].length() == 1){ buffer.append(splits03[splits03.length - 2]); buffer.append(\"_\"); buffer.append(splits03[splits03.length - 1]); }else { buffer.append(splits03[splits03.length - 1]); } return buffer.toString(); }} 5.打包上传生成UDF函数执行sql5.1上传到$HIVE_HOME/auxlibCREATE TEMPORARY FUNCTION parseClass AS 'com.cj.bigdata.hive.hiveWork.UDFParseClassCode'; 5.2sql完成需求select parseClass(class_str) class_code, count(*)from class group by parseClass(class_str) limit 10 ; 结果： 123 801123_0 759123_1 773123_2 7494354 71843541 72143541_0 72843541_1 75943541_2 75443542 728","permalink":"https://uniteready.github.io/2018/06/30/UDFToTopN%E4%BD%9C%E4%B8%9A/","photos":[]}],"categories":[],"tags":[{"name":"Javascript","slug":"Javascript","permalink":"https://uniteready.github.io/tags/Javascript/"},{"name":"hive","slug":"hive","permalink":"https://uniteready.github.io/tags/hive/"},{"name":"udf","slug":"udf","permalink":"https://uniteready.github.io/tags/udf/"}]}